Text,Topic,Source,Type,Year,Authors
"This paper proposes a single-bit consensus with ability to converge in finite-time. The main feature of this consensus protocol is the single-bit information update. The consensus protocol is proposed based on the sign of difference between state values. This implies that only single-bit of information (the sign) is required to update the state of the agents. This reduces the amount of processing/computation load and/or network communication load at each agent. It should be emphasized that in applications with real-time data processing where the computation and communication are required in faster time scale, the less computation and/or communication load is a significant merit. Since the protocol is nonlinear, a new Lyapunov function is proposed to prove consensus stability. Further, it is proved that for the proposed single-bit consensus protocol the Lyapunov function vanishes in finite-time, implying the finite-time convergence of the consensus protocol. In [4]–[6] finite-time consensus protocols are proposed, however these protocols impose large amount of computation on agents and are computationally less efficient than the proposed single-bit protocol in this paper.
Quantized consensus [12], [26], [27] is a related concept, where the agents reach consensus on quantized information with finite quantization levels for possibly unbounded data. In this direction [27] investigates a deterministic quantization based on alternating direction method of multipliers (ADMM). In [12] authors adopt a single-bit quantized consensus method for detection based on Bayesian criterion and Neyman-Pearson criterion. In [13] the authors propose two consensus+innovation type distributed detectors based on Generalized Likelihood-Ratio Test (GLRT) for composite hypothesis testing via a group of sensors. Further, communication in multi-agent systems based on single-bit of information is also adopted in distributed detection [28], [29]. In these works a group of sensors are spatially distributed over a surveillance field to locally detect the existence of an uncooperative target and then communicate their single-bit decisions to a fusion center. The single-bit decisions are based on either the hybrid combination of GLRT and Bayesian estimation [29] or Generalized-Rao Test [28]. The fusion center combines the received information based on the fusion rules and makes a global decision.
The main contributions of this paper are as follows: (i) the proposed protocol is based on single-bit of information, which makes it practical in real-time system applications. This is the most important feature of our proposed protocol. (ii) a new Lyapunov function is proposed to prove the stability and convergence of this nonlinear consensus protocol under certain connectivity condition.","Systems and Control, Single-Bit Consensus with Finite-Time Convergence: Theory and Applications",arxiv.org,Journal Paper,"2020, Jan 1",Mohammadreza Doostmohammadian
"Despite great advances in the theory of nonlinear controls, when it comes to practical implementation, PD control laws undisputably continue to be the most popular choice. [33, Table 1A] shows a detailed account on the list of controllers used and their corresponding acceptance ratings. The popularity of this type of control laws arises from its ease of implementation and robustness due to its model independent nature. This popularity has equally pervaded robotic systems. In fact, there are formal guarantees of stability for a broad class of robots that includes manipulators [21,36]. See Table 1, which shows a list of stability results. A more detailed list of these types of control laws and the corresponding stability results are given in [5, Table 1.1].
Despite the increase in complexity of the models, PD control laws have dominated even the domain of bipedal robots. Bipedal locomotion is hybrid in nature, with alternating phases of continuous (swinging forward of the nonstance foot) and discrete events (instantaneous impact of the nonstance foot with ground). In addition, unlike the industrial robotic arms, which have a fixed base, bipedal robots are largely underactuated. Fig. 1 shows some examples of bipedal robots that used PD and PD based tracking control laws. It is worth noting that the reference trajectories that were tracked were obtained offline. In fact, the field of locomotion has largely focused on experimental realization by dividing the problem into two parts. First, obtain reference trajectories/gaits in a simulation model by using offline optimization tools [11,31]. Second, play these trajectories in the robot by a low level tracking control law [12,32,40]. Due to uncertainties in the system, model based controllers were generally avoided, thereby giving preference to the more traditional PD based control laws. These control laws are known to be “hassle free”, since they are model-independent and easy to implement. Therefore, the main goal of this paper is to explore the stability properties of PD control laws for walking robots, which include varying levels of complexity due to the presence of impacts and underactuations.
We will be establishing stability of PD control laws by the construction of strict Lyapunov functions developed by Arimoto et. al. [2], Koditschek [18], and Bayard and Wen [36] all in the same period of time 1984-1988. Local and global stability results were shown for both stationary and time varying desired configurations [15,19,38], but only for fully actuated systems.","Robotics, Local Stability of PD Controlled Bipedal Walking Robots",arxiv.org,Journal Paper,"2020, Jan 1",Shishir Kolathaya
"A major challenge in the analysis of temporal networks is that one often has to discretize time by aggregating connections into time windows. Given a discrete or discretized set of interactions, one can then analyze communities, important nodes, and other facets of such networks by examining a multilayer-network representation of these interactions [1], [32], [40], [75]. An important challenge that arises with aggregation is that there may not be any obvious or even any ‘correct’ size of a time window (even when such aggregation employs non-uniform time windows [12], [68], [69], [74]). A window that is too small risks missing important network structures (e.g., by construing a signal as noise), but using an overly large window may obscure important temporal features. (See [18] for one discussion.) Moreover, in many social systems, interactions are bursty [4], [33], [41], which is a crucial consideration when aggregating interactions [31] and is potentially a major cause of concern when using homogeneous time windows [69]. Bursty interactions not only present a challenge when choosing the width of the time windows, but they also challenge where to place the boundaries such windows. Shifting time windows forward or backward may significantly alter the statistics of a data set, even when one does not change the width of the windows [41].
From a modeling perspective, aggregating interactions often may not be an appropriate approach for systems with asynchronous activity or which evolve continuously in time. See [79] for an investigation of biological contagions, [84] for a study of influential users in social networks, [85], [86] for a generalization of the formalism of ‘activity-driven networks’ to continuous time, [57] for a study of rankings in competitive sports, and [19] for a general continuous-time framework for temporal networks. In many cases, contacts in a temporal network can have a noninstantaneous duration, and it can be important to take such information into account [61], [71]. For example, the phone-call data that were studied in [28] require contacts to exist for the duration of a phone call. In other cases, interactions can be instantaneous (e.g., a mention in a tweet, a text message, and so on), and their importance decreases over time [11], [46]. For many types of temporal networks (e.g., feeds on social media), there is also a decay in attention span for reacting to posts [30], [49], [50].","Physics and Society, Tie-decay networks in continuous time and eigenvector-based centralities",arxiv.org,Paper,"2021, Jan 1","Walid Ahmad, Mason A. Porter, Mariano Beguerisse-Díaz"
"Image and video upscaling has been studied for decades and remains an active topic of research because of constant technological advances in digital imaging. One scenario where upscaling is now more demanding arises in digital display technologies, where new standards like BT.2020 [1] are introduced. The resolution of digital displays has experienced a tremendous growth over the past few decades, as shown in Figure 1. The transition between different formats leads to a challenging problem. On one hand, large amount of digital content still exist in popular old standards such as standard– definition (SD). On the other hand, the latest display technologies (e.g. 4K, 8K and above) are expected to show this content with reasonable quality. Standard upscaling technologies are clearly insufficient for this purpose. While a 2× upscaler maps 1 input pixel into 4 output pixels, an 8× upscaler maps 1 input pixel into 64 output pixels, which already can contain a small image. The problem demands advanced solutions that are capable of understanding the content and filling in these large pieces of images with visually appealing and consistent information. In particular, large upscaling factors are needed to convert SD to ultra high–definition (UHD) resolutions. Thus, large upscaling represents a real problem in current market and it is expected to persist and become even more challenging with the rapid development of new technologies.
In classical interpolation theory, upscaling images by integer factors is explained as two sequential processes: upsample (insert zeros) and filter [2], [3]. Standard upscaler algorithms, such as Bicubic or Lanczos, find high–resolution images with a narrow frequency content by using fixed low–pass filters. Modern tensor processing frameworks (e.g. Pytorch, Tensorflow, etc.) implement this process using a so–called strided transposed convolutional layer. Similarly, the image acquisition process can be modeled as: filter and downsample (drop samples). Many times we know the explicit model, e.g. bicubic downscaler. Tensor processing frameworks implement this process using a strided convolutional layer.
More advanced upscalers have followed geometric principles to improve image quality. For example, edge–directed interpolation uses adaptive filters to improve edge smoothness [4], [5], or bandlet methods use both adaptive upsampling and filtering [6]. Later on, machine learning has been able to use examples of pristine high–resolution images to learn a mapping from low–resolution [7]. The rise of deep–learning and convolutional networks in image classification tasks [8] quickly saw a series of important improvements in image resizing with large upscaling factors, which is the process widely know as image super–resolution (SR).","Image and Video Processing, Multi-Grid Back-Projection Networks",arxiv.org,Journal Paper,"2021, Jan 1","Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu, Xingqun Jiang"
"As Nyman and Ormerod (2017) note, over the past fifty years or so, a track record of macroeconomic forecasts and their accuracy has been built up on forecasting accuracy, especially with respect to the one year ahead predictions for growth.
A particular problem is the very poor record of predicting recessions. The failure to forecast the financial crisis recession of the late 2000s is well known. But this is a constant theme in the forecasting record over the past 50 years.
For example, four quarters ahead, the mean prediction of the large number of forecasters reported in the Survey of Professional Forecasters has never been for negative growth. In contrast, since 1968 there are 23 examples in the actual data of US real GDP growth being less than zero in a quarter.
Nyman and Ormerod (op.cit.) show that applying a random forest algorithm to a small set of monetary variables generates four-quarter ahead predictions of real US GDP growth which are much better over the 1990-2017 period than those reported by the Survey of Professional Forecasters. Similar results are obtained for the UK.
The machine learning algorithm captures the shallow recession of the early 1990s, the marked slow down (although not technically a recession) in the US economy in the early 2000s after the dotcom crash, and the recession of 2008/09. It does not predict a recession when one did not occur.
In this paper, we extend the previous results on forecasting the Great Recession by examining the contributions made to the Great Recession of the late 2000s by each of the small set of financial variables used in the prediction models. We disaggregate private sector debt into its household and non-financial corporate components.
Section 2 describes the methodology, and section 3 the results.
As described in Nyman and Ormerod (op.cit.), we try to replicate as closely as possible an actual forecasting situation.
The dependent variable in our analysis is the third estimate of real GDP growth in the relevant quarter, rather than the most recent estimate which is now available. The third estimate is in general the one on which policy makers would rely when trying to judge the current state of the economy at the time.
Friedman and Schwartz (1963) argued that the Federal Reserve’s monetary policies were largely to blame for the severity of the Great Depression. We therefore selected, on this a priori theoretical basis, a small set of monetary variables as potential explanatory factors. We selected them without any preliminary analysis as to whether they would be of any practical use in predicting recessions.","General Economics, Understanding the Great Recession Using Machine Learning Algorithms",arxiv.org,Paper,"2020, Jan 2","Rickard Nyman, Paul Ormerod"
"The Regression Discontinuity (RD) design is widely used in program evaluation, causal inference, and treatment effect settings. (For general background on these settings, see Imbens and Rubin (2015) and Abadie and Cattaneo (2018), and references therein.) In recent years, RD has become one of the prime research designs for the analysis and interpretation of observational studies in social, behavioral, biomedical, and statistical sciences. For introductions to RD designs, literature reviews, and background references, see Imbens and Lemieux (2008), Lee and Lemieux (2010), Cattaneo and Escanciano (2017), and Cattaneo, Idrobo, and Titiunik (2019, 2020). Modern empirical work in RD designs often employs a mean square error (MSE) optimal bandwidth for local polynomial estimation of and inference on treatment effects.1 This MSE-optimal bandwidth choice yields an MSE-optimal RD point estimator, but is by construction invalid for inference. Robust bias corrected (RBC) inference methods provide a natural solution to this problem: RBC confidence intervals and related inference procedures remain valid even when the MSE-optimal bandwidth is used (Calonico, Cattaneo, and Titiunik, 2014; Calonico, Cattaneo, Farrell, and Titiunik, 2019). In this paper, we show that this choice of bandwidth is suboptimal when the goal is to construct RBC confidence intervals with minimal coverage error (CE), and we establish a new bandwidth choice delivering CE-optimal RBC confidence interval estimators or, analogously, minimizing the error in rejection probability of the associated hypothesis testing procedures for RD treatment effects.
Our main results are valid coverage error expansions for local polynomial RBC confidence interval estimators. The precise characterization offered by these expansions allows us to study bandwidth selection in detail, and to propose several novel bandwidth choices that are optimal for inference. First and foremost, we derive a CE-optimal bandwidth choice designed to minimize coverage error of the interval estimator, which is a fundamentally different goal than minimizing mean square error of the point estimator. The MSE- and CE-optimal bandwidths are therefore complementary, as both can be used in empirical work to construct, respectively, optimal point estimators and optimal inference procedures for RD treatment effects. For example, we find that in the case of the popular local linear RD estimator, if the sample size is n = 500, then shrinking the MSE-optimal bandwidth by approximately 27% leads to RBC confidence intervals with the fastest coverage error decay rate. Further, we use our expansions to derive bandwidth choices that trade off coverage error against interval length, which is conceptually analogous to trading size and power of the associated statistical tests, while retaining asymptotically correct coverage (or size control).","Econometrics, Optimal Bandwidth Choice for Robust Bias Corrected Inference in Regression Discontinuity Designs",arxiv.org,Journal Paper,"2020, Jan 3","Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell"
"A core goal of causal inference is to identify and estimate effects of a treatment variable on an outcome variable. A common assumption used to identify such effects is unconfoundedness, which says that potential outcomes are independent of treatment conditional on covariates. This assumption is also known as conditional independence, selection on observables, ignorability, or exogenous selection; see Imbens (2004) for a survey. This assumption is not refutable, meaning that the data alone cannot tell us whether it is true. Nonetheless, empirical researchers may wonder: How important is this assumption in their analyses? Put differently: How sensitive are their results to failures of the unconfoundedness assumption?
A large literature on sensitivity analysis has developed to answer this question. Moreover, researchers widely acknowledge that answering this question is an important step in empirical research. For example, in their figure 1, Caliendo and Kopeinig (2008) describe the workflow of a standard analysis using selection on observables. Their fifth and final step in this workflow is to perform sensitivity analysis to the unconfoundedness assumption. Imbens and Wooldridge (2009, section 6.2), Imbens and Rubin (2015, chapter 22), and Athey and Imbens (2017) all also recommend that researchers conduct sensitivity analyses to assess the importance of non-refutable identifying assumptions. In particular, Athey and Imbens (2017) describe these methods as “a systematic way of doing the sensitivity analyses that are routinely done in empirical work, but often in an unsystematic way.”
Most of the existing approaches to assessing unconfoundedness rely on strong auxiliary assumptions, however. For example, they often assume treatment effects are homogeneous and that all unobserved confounding arises due to a single unobserved variable whose distribution is parametrically specified, like a binary or normal distribution. They also often assume a parametric functional form for potential outcomes, like a logit model for binary potential outcomes or a linear model for continuous potential outcomes. These assumptions—which are not needed for identification of the baseline model when unconfoundedness holds—raise a new question: Are the findings of these sensitivity analyses themselves sensitive to these extra auxiliary assumptions?
In this paper, we provide a set of tools for assessing the sensitivity of the unconfoundedness assumption which do not rely on strong auxiliary assumptions that are not used for the baseline analysis. We do this by studying nonparametric relaxations of the unconfoundedness assumption. Specifically, we apply the identification results of Masten and Poirier (2018), who consider a class of assumptions called conditional c-dependence.","Econometric, Assessing Sensitivity to Unconfoundedness: Estimation and Inference",arxiv.org,Paper,"2020, Dec 31","Matthew A. Masten, Alexandre Poirier, Linqi Zhang"
"Inference under partial identification is by now the subject of a broad literature.1 Only recently did attention turn to the following concern: If a partially identified model is misspecified, this may manifest in either an empty or –and arguably worse– in a misleadingly small confidence region. That is, misspecified inference can be spuriously precise.
The reason is that most confidence regions used in partial identification invert tests of H0 : θ ∈ ΘI ; here, θ is a parameter and ΘI is the identified set. If H0 is rejected at every θ, the confidence region is empty. If H0 is barely not rejected at a few parameter values, the confidence region may be very small. This issue is empirically relevant. For example, an empty sample analog of ΘI occurs in de Quidt, Haushofer, and Roth (2018), whose inquiry sparked the present research and whose data are reanalyzed below.
The literature on this issue is still young. Ponomareva and Tamer (2011) provide an early diagnosis. Kaido and White (2013) propose a notion of pseudotrue identified set and an estimator thereof. Molinari (2020) explains the issue in detail and highlights it as important area for further investigation. The most thorough treatment is by Andrews and Kwon (2019), who emphasize the issue’s importance and provide a general inference method that avoids spurious precision and ensures coverage of a pseudotrue identified set.
The present paper is in the spirit of Andrews and Kwon (2019). I focus on the simple but empirically salient case of a scalar parameter with upper and lower bounds whose estimators are jointly asymptotically normal. That is, I revisit the setting of Imbens and Manski (2004, without their superefficiency assumption) and Stoye (2009). For this setting, I propose a confidence interval with the following features:
 • It is never empty nor very short (a lower bound on its length is reported later).
 • It exhibits asymptotically guaranteed coverage uniformly over the identified set and additionally for a well-defined pseudotrue parameter.
 • It tends to be shorter than more conventional intervals in benign cases, including in the empirical application.
 • It is free of tuning parameters and trivial to compute.
For target coverage of 95% and for the special case of uncorrelated estimators, e.g. in this paper’s empirical application, the confidence interval can be verbally defined as follows:
 • Add ±1.64 standard errors to estimators of upper and lower bounds.","Econometrics, A Simple, Short, but Never-Empty Confidence Interval for Partially Identified Parameters",arxiv.org,Paper,"2020, Dec 31",Jörg Stoye
"Statistical data analysis that disentangles the causal factors of data formation and computes a representation that facilitates the analysis, visualization, compression, approximation, and/or interpretation of the data is challenging and of paramount importance.
“Natural images are the composite consequence of multiple constituent factors related to scene structure, illumination conditions, and imaging conditions. Multilinear algebra, the algebra of higherorder tensors, offers a potent mathematical framework for analyzing the multifactor structure of image ensembles and for addressing the difficult problem of disentangling the constituent factors or modes.” (Vasilescu and Terzopoulos, 2002 [69])
Scene structure is composed from a set of objects that appear to be formed from a recursive hierarchy of perceptual wholes and parts whose properties, such as shape, reflectance, and color, constitute a hierarchy of intrinsic causal factors of object appearance. Object appearance is the compositional consequence of both an object’s intrinsic causal factors, and extrinsic causal factors with the latter related to illumination (i.e. the location and types of light sources), imaging (i.e. viewpoint, viewing direction, lens type and other camera characteristics). Intrinsic and extrinsic causal factors confound each other’s contributions hindering recognition.
“Intrinsic properties are by virtue of the thing itself and nothing else” (David Lewis, 1983 [42]); whereas an extrinsic properties are not entirely about that thing, but as result of the way the thing interacts with the world. Unlike global intrinsic properties, local intrinsic properties are intrinsic to a part of the thing, and it may be said that a local intrinsic property is in an “intrinsic fashion”, or “intrinsically” about the thing, rather than “is intrinsic” to the thing [21, 31]. David Lewis [42] provides a formal discussion of intrinsic and extrinsic concepts of causality and addresses a few related distinctions that an intuitive definition conflates, such as local versus global intrinsic properties, duplication preserving properties, and interior versus exterior properties. The meaning of intrinsic and extrinsic causation was extensively explored in philosophy, philosophy of mind, metaphysics and philosophy of physics [32, 42, 43, 46, 52].
Our goal is to explicitly represent local and global intrinsic causal factors as statistically invariant representations to all other causal factors of data formation.
Historically, statistical object recognition paradigms can be categorized based on how object structure is represented and recognized, ie. based on the appearance of an object’s local features [22, 75], or based on the overall global object appearance [2, 4, 26, 45, 56, 62, 78]. Both approaches have strengths and shortcomings.","Computer Vision and Pattern Recognition, Compositional Hierarchical Tensor Factorization: Representing Hierarchical Intrinsic and Extrinsic Causal Factors",arxiv.org,Journal Paper,"2020, Jan 1","M. Alex O. Vasilescu, Eric Kim"
"A major challenge in computer simulation of complex systems is to choose suitable model parameters. These parameters usually represent specific intrinsic attributes of the system. The input values of the model parameters can significantly affect the accuracy and usefulness of the computer output. When physical observations are available, one can adjust the computer model parameters so that the computer outputs match the physical data. We call this activity the calibration of computer models.
The celebrated Bayesian calibration method by Kennedy and O’Hagan [10] is one of the major and widely used approaches for the calibration of computer models. A remarkable contribution of [10] is to incorporate a “discrepancy function” to model the difference between the computer outputs and the physical process. This discrepancy does exist in most computer simulation problems, because we have to resort to simplifications and unrealistic assumptions when building the computer models.
Without an informative prior, the Kennedy and O’Hagan model is non-identifiable, because one cannot determine the model parameters and the discrepancy function simultaneously. Kennedy and O’Hagan [10] used a Gaussian process as a prior for the discrepancy function. Tuo and Wu [22] conducted a theoretical study on a simplified version of the Kennedy-O’Hagan method (abbreviated as the K-O method) when the physical data are noiseless. Under this condition, the radial basis functions approximation can be regarded as a frequentist version of Gaussian process regression. With the help of related mathematical tools, Tuo and Wu [22] identified the limit value of the Kennedy-O’Hagan method as well as the rate of convergence.
A primary goal of this work is to establish an asymptotic theory for the K-O method with noisy physical data. The frequentist version of the Gaussian process regression, in this situation, is the kernel ridge regression [15]. With an improved rate of convergence for kernel ridge regression, we prove that, under certain conditions, the K-O estimator tends to the parameter value which minimizes the norm of the residual function in the reproducing kernel Hilbert space. We also present the rate of convergence of the K-O estimator. As a consequence, we relax a key and rather restrictive assumption in [22]. Tuo and Wu had to assume that the physical experiments have no random errors, which is not realistic.
There is a vast literature on the theoretical properties of ridge kernel regression. It is known that the rate of convergence of this method can be improved by imposing extra smoothness conditions on the underlying function; see, e.g., [8].","Statistics Theory, On the Improved Rates of Convergence for Matérn-type Kernel Ridge Regression, with Application to Calibration of Computer Models",arxiv.org,Paper,"2020, Jan 1","Rui Tuo, Yan Wang, C. F. Jeff Wu"
"Lovasz [22] introduced the ϑ function of a graph as an upper bound on the Shannon capacity – the independence number regularized under the strong graph product. The ϑ quantity is an upper bound on independence number, a lower bound on fractional chromatic number, and is multiplicative under the strong and the disjunctive graph products. It is a semidefinite program, hence efficiently computable both in theory and in practice. It is monotone under graph homomorphisms [8]; in fact its bound on independence and chromatic number follow from this.
Further insight into ϑ is gained by allowing vertices to be weighted [16, 18]. Weights are basically equivalent to duplicating vertices [18] except that weights don’t have to be whole numbers. Aside from only being defined for non-negative weights, the weighted ϑ of a graph resembles a norm on the weight vector: it scales linearly and is convex. In that language, ϑ of the complement graph is the dual norm. The set of weights w for which ϑ(G, w) ≤ 1 is investigated in [16], where facets of this convex body are shown to correspond to clique constraints. This set is polyhedral if and only if the graph is perfect.
Lovasz’s bound can be adapted to quantum channels via a suitable generalization of graphs where an operator subspace takes the place of the adjacency matrix [10]. These so called non-commutative graphs have since drawn interest in connection with quantum channels but also independently of any application. Several classical graph definitions and results carry over to non-commutative graphs, including homomorphisms [5, 25, 27, 30], chromatic numbers [17, 19, 27], Ramsey and Turan theorems [31, 32], asymptotic spectrum [20], a Haemers bound [15], and connectivity [6]. It can happen that there are multiple ways to generalize a particular concept: [3] presents two generalizations of ϑ distinct from the one in [10] (though possibly the same as each other).
The present work investigates a weighted version of the ϑ˜ of [10], generalizing most of the results from [16]. We note that [3] defined a weighted version of their ϑ generalization, and investigated the corresponding theta body. It is not known whether it supports the sort of duality relations we find in this paper.
We will cover basic notation in section II, introduce our weighted ϑ˜ in section III, prove a duality relation in section IV, extend this to S0-graphs in section V (with the core proof deferred to appendix A), and in section VI explore the geometry of the theta body for non-commutative graphs and its relation to perfect graphs.","Combinatorics, Weighted theta functions for non-commutative graphs",arxiv.org,Paper,"2021, Jan 1",Dan Stahlke
"Related work. In the literature, there has been rapid development of different methods for simulating gradient flow models including (1.1), see e.g., [8, 9, 18, 26, 42, 12, 49, 50, 43, 53, 54, 46, 6]. They vary either in the spatial discretization or the time discretization, while the latter typically emphasizes preserving the energy dissipation property with no or mild time step restrictions. Let us briefly discuss existing works closely related to what we do here.
DG spatial discretization. It is known that for equations containing higher order spatial derivatives, DG discretization entails subtle difficulties in defining numerical fluxes. Several approaches have been developed to deal with the difficulties, including the local DG (LDG) methods [52, 11, 47], the mixed symmetric interior penalty (SIPG) methods [13, 14, 48, 16], and the ultraweak DG [7]. To avoid certain drawbacks of these methods, a penalty free DG method was introduced in [29], where the symmetric structure of the model (1.1) is essentially used. This method still inherits the advantages of the usual DG methods, such as higher accuracy, flexibility in hp-adaption, capacity to handle domains with complex geometry [25, 22, 41, 44], its distinct feature lies in numerical fluxes without using any interior penalty. This is the spatial discretization we shall follow in this work.
EQ reformulation and time discretization. To keep the energy stability for gradient flow models, several time discretization techniques are available in the literature, including the so-called convex splitting [12, 49], and the stability approach [50, 43]. The former leads to nonlinear schemes, and the later often imposes restrictions on nonlinear terms in the model. The energy quadratization (EQ) approach introduced in [53, 54] turned to be rather general that it can be applied to a class of gradient flow models if the free energy is bounded below. Based on the idea of EQ, the scalar auxiliary variable (SAV) approach was introduced later in [46], where linear systems only with constant coefficients need to be solved. Several extensions of EQ and SAV have been further explored in [5, 23, 34, 51]. Earlier EQ based schemes are mostly up to 2nd order accurate in time, until recent works [20, 21], where the EQ formulation is combined with the Runge-Kutta methods to achieve high order in time schemes. Note that their schemes are fully nonlinear so that the solution existence and uniqueness are not guaranteed for large time steps.","Numerical Analysis, Energy stable Runge-Kutta discontinuous Galerkin schemes for fourth order gradient flows",arxiv.org,Paper,"2021, Jan 1","Hailiang Liu, Peimeng Yin"
"Almost 100 years have passed since the release of the book with the striking title ""The Meaning of Meaning"" [1], in which authors Ogden and Richards first proposed some formalism when describing such a complicated concept as ""meaning."" Their approach's success is based on the fact that from all the variety of problems associated with human consciousness, they singled out only one problem – the question of meaning, leaving aside a significant number of other challenges.
The famous ""Ogden-Richards triangle,"" as it turned out later, could be traced back to the works of Aristotle. Of course, this does not diminish Ogden and Richards' achievements but only emphasizes that the problem of consciousness in general, and meaning in particular, has been known for at least two and a half millennia. This topic's interest is so great that practically all well-known philosophers and scientists have not ignored it. Here, we note the works of only those who are impossible not to mention – Descartes [2] and Penrose [3].
Each branch of science builds its models of consciousness and its models for extracting meaning from the information received. Biologists concentrate on the chemical reactions in the human brain; mathematicians make informational models of the brain; psychologists and philosophers use their specific methods that are poorly understood by representatives of the ""exact sciences."" The theory of ""quantum consciousness"" [4] and, of course, its criticism [5] has become popular now. A common feature of these studies is that they are directed to the micro-world; that is, they try to explain consciousness in terms of the dynamics of the ""elementary particles"" of the brain – neurons.
At the same time, the question remains about the work of the brain, not at the microlevel, but at the macro-level – at the level of an ensemble of a colossal number of certain ""building blocks,"" which, presumably, reside in the human brain, and whose microdynamics provide processing, comprehension, and storage of incoming information. If ""building blocks"" are understood as neurons, then there are about a billion of them in the human brain – a large enough number to speak of a statistical ensemble. Similar problems are posed in statistical physics and thermodynamics, which describe the macrostates of a vast number of individual objects, of which surprisingly little is known of the internal structures.
This phenomenological work is an example of the Copenhagen school of quantum mechanics, the slogan of which is ""Shut up and compute,"" without going into fascinating, but so far inaccessible to our understanding, the actual mechanisms of the brain at the micro-level.","General Physics, Dynamics of Meaning",arxiv.org,Paper,2020. Dec 21,Gary Gindler
"The field of plasmonics is concerned with the manipulation and confinement of electromagnetic energy in nano-scale geometries. This is generally accomplished with systems consisting of Au or Ag nanomaterials optically excited by surface plasmon resonance (SPR). In the case of thin metal films, this excitation takes the form of electron density waves propagating along the metal-dielectric interface. In the case of metal nanoparticles, size, shape, and local dielectric environment determine the conditions for the so-called localized surface plasmon resonance (LSPR), in which conduction electrons oscillate coherently with the driving field inside the volume of the nanoparticle.
In general for plasmon surface resonance there is significant enhancement of the local electric field at the metal-dielectric interface. For an incident electromagnetic wave with field E0, the local electric field at the metal surface on resonance ESP can be enhanced by factors ESP/E0 of anywhere from 5-10 for Au thin films, to factors of 200 or more for sharp-tipped nanostructures such as Au nano-bipyramids (NBP’s).
The fundamental physical cause for the observed electric field enhancement is the bunching of conduction electrons at the metal surface driven by the incident field. The electrons accumulate at sharp dielectric discontinuities, such as the surface of the film in the case of SPR or the edges or tips of nanoparticles in LSPR, at every cycle in phase with the driving optical field. This coherent oscillation of the conduction electrons in the metal may persist for some 10’s of femtoseconds after the driving field has ceased, followed by thermalization to a so-called “hot electron” distribution.
When combined with ultrafast (femtosecond pulsed) laser systems delivering high peak powers, plasmonic nanomaterials can exhibit surface electric fields of 109 V/m or higher. These large fields can give rise to a number of nonlinear optical effects that have been observed, including second harmonic generation (SHG), ponderomotive acceleration of electrons, and other second- and higher-order nonlinearities.
In particular, evidence for significant ponderomotive forces produced by plasmon resonances have been shown in several previous reports. Irvine and Elezabbi demonstrated that the excitation of a thin Ag film with an ultrafast laser pulse will allow electrons to tunnel through the metal surface and become accelerated to keV energies via the ponderomotive force. A similar report by Dombi et. al. shows the emission and acceleration of electrons from lithographically-fabricated Au nanoparticles.
The unique aspect of surface plasmon resonance phenomena to concentrate and manipulate electrons in nanoscale volumes has found applications in areas such as biosensing, cancer therapy, and solar energy conversion.","General Physics, Ponderomotive Screening of Nuclear Fusion Reactions Based on Localized Surface Plasmon Resonance",arxiv.org,Paper,"2020, Oct 30","Mason J. Guffey, Alfred Y. Wong"
"Most of the models and approaches to the pentaquark states in the references discussed above rely on an assumption that a pentaquark state under consideration has certain structure in color, spin, and flavor. Considering the interpolating current to a pentaquark state for analysis within the QCD sum-rules (SRs), the clustering in the color, flavor and spin space is inevitable due to the absence of the invariant rank-5 tensors for the color, flavor and spin subspaces. For example for SU(3)c, the largest rank of invariant tensor is 3, therefore considering two-quarks and three-quarks cluster would be one of the natural possibilities in constructing the interpolating for a pentaquark state. Recently, the hidden-charm pentaquark state of the quark content udscc with the flavor singlet structure in SUF(3) (the flavor singlet hiddencharm pentaquark) was considered in [8] within quarks models. Specially, the flavor singlet hidden-charm pentaquark was analyzed as the bound state of a three-quark and two-quark parts both in color octets, and the stable result was got for the total spin 1/2 in [8].
In this paper, we study first the flavor singlet hiddencharm pentaquark states of udscc with the spins 1/2, 3/2, and 5/2 using QCD SRs. We assume that these states consist of two colored clusters as discussed in [8, 30, 31] within quark models. So, we consider them as states consisting of the three-quark cluster uds and the two-quark cluster cc. Additionally, we assume that all quarks are in an S-wave, the colors of both clusters are color octets, and the two-quark cluster has spin 1 since it has been shown in [8] that such clusters of uds and cc yielded the most stable result. To check this assumption, we consider the pentaquark states containing a scalar two-quark cluster and find that such states lead to higher masses than those obtained from pentaquarks with a two-quark cluster of spin 1. Then, we also examine other possible pentaquark states containing the two clusters udc-cs and usc-cd by extending the results of the flavor singlet uds and cc case. Furthermore, the pentaquark states of the two color-octet clusters udc-cu are studied to see if any of the states observed by LHCb could be understood in terms of pentaquark state with color-octet substructure and flavor-singlet flavor three-quark part udc. Let us point out that the method of QCD SR relies on the local current for studying the spectroscopy of hadrons.","High Energy Physics - Phenomenology (hep-ph), Hidden charm pentaquarks with color-octet substructure in QCD Sum Rules",arxiv.org,Journal Paper,"2020, Jan 1","Alexandr Pimikov, Hee-Jung Lee, Pengming Zhang"
"The ability to control ion and solute transport through membranes is central to many processes in chemistry, biology and materials science, such as water desalination, chemical separation of gases, ions, organic solvents, and viruses, and in vivo transport of ions, pharmaceuticals and nutrients through biological membranes and channel proteins. Most such applications rely on the semipermeability of the underlying membrane, i.e., its ability to preferentially allow for the passage of some molecules and/or ions while excluding the majority of other components. Consequently, the need to improve solvent permeability and solvent-solute selectivity of membranes has been extensively addressed in recent years. The major obstacle to enhancing solute-solute selectivity is the considerable gap in our understanding of the molecular-level features that control selectivity. This is primarily due to insufficient spatiotemporal resolutions of most experimental techniques in probing the molecular mechanism of solvent and solute transport through nanopores. In principle, it is generally understood that the selectivity of a nanoporous membrane for a specific solute is mainly dictated by steric, charge-exclusion and solvation effects. More recent investigations, however, point to a more complex picture, and underlines the importance of other more subtle factors such as polarizability effects and mechanical properties of the membrane. Consequently, selectivity is affected by a complex interplay of a confluence of factors, and cannot be determined by single measures, such as ion size, charge density or hydration energy, as demonstrated in recent experimental studies. Understanding how membrane structure affects its selectivity for different types of solutes is therefore of pressing importance.
In recent years, there has been an increased interest in using molecular simulations to study solvent and solute transport through nanoporous membranes, as molecular simulations have been utilized for computing properties such as solvent permeability, free energy barriers, and solute rejection rates across numerous well-defined nanoporous membranes. However, these studies either employ conventional techniques such as molecular dynamics (MD), which provide unbiased kinetic and mechanistic information but cannot efficiently probe long solute passage timescales, or utilize techniques such as umbrella sampling that are based on applying biasing potentials along pre-specified reaction coordinates, but provide no direct information about kinetics. Therefore, such traditional techniques are inadequate for comprehensively investigating the structure-selectivity relationship in ultra-selective membranes due to their limited range of accessible timescales or their inability to probe the passage kinetics altogether.","Soft Condensed Matter, Induced Charge Anisotropy: a Hidden Variable Affecting Ion Transport through Membranes",arxiv.org,Paper,"2020, Jan 1","Hessam Malmir, Razi Epsztein, Menachem Elimelech, Amir Haji-Akbari"
"Living in a group has considerable impact on an individual’s life. Communication, both within and between groups is imperative for group living to be sustained. Vocalizations are primarily used in social interactions by higher order organisms, and tend to attract the attention of humans due to their similarity to the most common means of human communication – speech. Humans are probably the only species to have evolved a multitude of complex linguistic systems for communication. However, other species like honeybees, dolphins, elephants etc. have been found to use complex communication that are akin to languages (Seyfarth & Cheney, 2010). Communication in animals, however, is not limited to vocalizations and can be acoustic, visual, olfactory and tactile. How individuals communicate with conspecifics and with other species and how these systems vary from human communication systems, are questions that continue to intrigue us, leading to a large body of research (Marler P, 1961). While communication is a prerequisite for evolving social behaviour, social interactions tend to shape the personalities of individuals, influencing the manner in which they communicate. For example, social interactions contextualize vocalizations (Yin, 2002) and may guide an individual’s usage of and response to vocalization, playing an essential role in the individual’s ability to communicate effectively (Rendall, Seyfarth, Cheney, & Owren, 1999);(De La Torre & Snowdon, 2002). The intricacies of vocal communication can be best studied in group living species that use various kinds of vocalizations in different social contexts.
Canids are good model systems for studying vocalizations, as they show different levels of social organization and actively communicate using vocalizations, though olfactory signals also play an important role in canid communication(Cohen & Fox, 1976). For example, within wolf packs, howling plays a critical role to reassemble separated individuals, as well as to communicate information on individual identity, location, and other behavioral and environmental factors (Theberge & Falls, 1967). Communication in dogs (Canis lupus familiaris) involves both their conspecifics and humans. Dogs have a broad range of vocal repertoire (Yeon, 2007). Although their vocalizations are quite similar to their close relatives, the gray wolf (Canis lupus lupus), dogs vocalize in a wider variety of social contexts as compared to wolves (Pongrácz, Molnár, & Miklósi, 2010). The vocal behaviour of dogs underwent considerable changes during the domestication process, which is considered to be a result of the dog’s adaptation to the human social environment (Feddersen-Petersen, 2000). Among the different vocalizations, the bark is undoubtedly the most typical of dogs.","Quantitative Methods (q-bio.QM), Free-ranging dogs do not distinguish between barks without context",arxiv.org,Paper,"2020, Jan 1","Prothama Manna, Anindita Bhadra"
"Humans tend to subjectively overestimate small probabilities (e.g., dying in an airplane crash, winning the Powerball jackpot lottery) and underestimate large probabilities (e.g., getting lung cancer from smoking cigarettes) when making decisions under risk and uncertainty. These subjective probabilities are typically modeled over the range [0,1] using a probability distortion (or weighting) function [4]-[7]. It is commonly assumed that subjective probability is represented in the brain in continuous form (i.e., a Real number that can take any value between 0 and 1). This assumption may result in inaccurate analytical findings if decisions are made using a non-continuous (i.e., finite precision) representation. For example, if an individual were to choose between a $100 lottery of a winning probability of 0.89721, versus a $150 lottery of a winning probability of 0.50219, s/he will typically round the numbers when making the choice [8]. The choice is then greatly simplified to a $100 lottery with a winning probability of 0.9, versus a $150 lottery with a winning probability of 0.5. Analysis performed using a continuous representation is, strictly speaking, inaccurate because 0.89721 is actually treated indifferently to 0.9. Rounding implies that a range of probabilities is treated as being the same (i.e., indifferent). This analytical inconvenience is often ignored because standard (i.e., continuous) models cannot easily take into account such indifference. While Kahneman and Tversky [8] expected rounding (i.e., “editing”), they did not quantify it.
Quantization [9] is the term describing an encoding process in which a continuous quantity (i.e., Real number or analog signal) is systematically divided into a finite number of possible categories. We note that the term “quantization” is loosely synonymous to terminologies such as discretization, chunking, categorization and classification. The term “discrete” is common in neuroscience, whereas the term “quantized” is more common in engineering. Here, we use both terms interchangeably. Rounding a number is the oldest example of quantization [10]. Oliver, Pierce and Shannon [11] employed quantization to convert continuous signals (e.g., audio/voice) to their digital (i.e., binary) forms to enable the maximum possible efficiency of data compression and communications [12], thus laying the theoretical foundations for almost all modern communications systems. For binary quantization, the total number of categories equals to 2n , where n is the quantization precision expressed in bits. For example, suppose that we are encoding probabilities (i.e., Real numbers in the interval [0,1]).","Neurons and Cognition (q-bio.NC), A Quantized Representation of Probability in the Brain",arxiv.org,Paper,"2020, Jan 1","James Tee, Desmond P. Taylor"
"Time-to-event data are frequently encountered in randomized clinical trials, especially in the cardiovascular and oncology therapeutic areas. Typically, such data are analyzed using the Cox proportional hazards (PH) model for treatment effect estimation and the log-rank test for hypothesis testing. While both methods perform well under the assumption of proportional hazards, bias and loss of power can occur when the underlying hazards are no longer proportional.
The scientific breakthrough of immuno-oncology in recent years has brought up unprecedented interest for time-to-event data with non-proportional hazards (NPH). Different types of non-proportionality have appeared in clinical trials, including and not limited to early/diminishing treatment effect, late/delayed treatment effect or crossing hazards due to their unique mechanisms. The first two scenarios are called quantitative NPH, where the hazard ratio is either ≥ 1 or ≤ 1 at all times, i.e., the direction of the treatment effect remains the same; the crossing hazards scenario is qualitative NPH, where the hazard ratio is < 1 at some time intervals, and > 1 at some other times, i.e., the direction of the treatment effect changes during the course of trials. Crossing hazards is usually considered a special case for non-proportionality that requires further considerations in terms of results interpretation. Several real-world examples of clinical trials for PH [1, 2], for NPH with delayed effects [3, 4] and for NPH with crossing hazards [5, 6, 7, 8] can be found in the literature.
Multiple methods of both testing and estimation of treatment effect have been developed under NPH. For treatment effect testing, the weighted log-rank [9], weighted Kaplan-Meier [10, 11], MaxCombo tests [12, 13], restricted mean survival time (RMST) method [14, 15], and the nonparametric K-sample omnibus non-proportional hazards (KONP) tests based on sample-space partition [16] have been proposed. For treatment effect estimation, weighted hazard ratio [17], piecewise hazard ratios [18, 19, 20], RMST [14, 15, 21], and milestone survival at given time points [22] have been studied. Each method has its own advantages and limitations. There is no general consensus on which method(s) should be applied to trial design and data analysis because many factors around the type of non-proportionality have an impact on the choice of method. In addition, under general multiple comparison scenarios or group sequential trial designs with multiple endpoints in clinical trials, small overall α levels (i.e., α ≤ 0.01) are frequently specified to control the family-wise error rate [23, 24].","Methodology (stat.ME), CauchyCP: a powerful test under non-proportional hazards using Cauchy combination of change-point Cox regressions",arxiv.org,Journal Paper,"2020, Dec 31","Hong Zhang, Qing Li, Devan V. Mehrotra, Judong Shen"
"Organisms that live in changing environments evolve strategies to respond to the fluctuations. Many such adaptations are reactive, e.g. sensory systems that allow detecting changes when they occur, and responding to them. However, adaptations can be not only reactive, but also predictive. For example, circadian clocks allow photosynthetic algae to reorganize their metabolism in preparation for the rising sun [1, 2]. Another example is the anticipatory behavior in E. coli, which allows it to prepare for the next environment under its normal cycling through the mammalian digestive tract [3]; similar behaviors have been observed in many species [4, 5].
All these behaviors effectively constitute predictions about a future environment: the organism improves its fitness by exploiting the regularities it “learns” over the course of its evolution [6]. Learning such regularities can be beneficial even if they are merely statistical in nature. A prime example is bet hedging: even if the environment changes stochastically and without warning, a population that learns the statistics of switching can improve its long-term fitness, e.g., by adopting persistor phenotypes with appropriate probability [7, 8]. The seemingly limitless ingenuity of evolutionary trial-and-error makes it plausible that virtually any statistical structure of the environment that remains constant over an evolutionary timescale could, in principle, be learnt by an evolving system, and harnessed to improve its fitness [9].
However, the statistical structure of the environment can itself change, and this change can be too quick to be learned by evolution. For example, an organism might experience a period of stability followed by a period of large fluctuations; or an environment where two resources are correlated, and then another where they are not. Note that our focus here is not the rapidity of fluctuations, but the slower timescale on which the structure of those fluctuations changes. One expects such scenarios to be particularly common in an eco-evolutionary context. As an example, consider a bacterium in a small pool of water (Fig. 1A). Its immediate environment, shaped by local interactions, is fluctuating on the timescale at which the bacterium changes neighbors. The statistical properties of these fluctuations depend on the species composition of the pool. As such, the fast fluctuations are partially predictable, and learning their structure could help inform the fitness-optimizing strategy: a neighbor encountered in a recent past is likely to be seen in the near future. However, these statistics change on an ecological timescale, and such learning would therefore need to be accomplished by physiological, rather than evolutionary, mechanisms.","Cell Behavior (q-bio.CB), A simple regulatory architecture allows learning the statistical structure of a changing environment",arxiv.org,Paper,"2020, Dec 31","Stefan Landmann, Caroline M. Holmes, Mikhail Tikhonov"
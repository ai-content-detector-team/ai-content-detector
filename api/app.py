#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              IMPORT DES LIBRAIRIES
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Union
import pandas as pd
import numpy as np
import re
from collections import defaultdict, Counter
import mlflow
import spacy
import nltk
from nltk.corpus import stopwords
from transformers import AutoTokenizer



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           DESCRIPTION COMPLETE DE L'API
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

description = """
## AI Review Detector API

L'**AI Review Detector API** est une solution professionnelle de dÃ©tection automatique des avis gÃ©nÃ©rÃ©s par intelligence artificielle.

### ğŸ¯ Objectif

Cette API permet d'identifier si un avis client ou une critique a Ã©tÃ© rÃ©digÃ© par un humain ou gÃ©nÃ©rÃ© par une intelligence artificielle, 
aidant ainsi les entreprises Ã  maintenir l'authenticitÃ© de leurs plateformes d'Ã©valuation.

### ğŸ”¬ ModÃ¨le de dÃ©tection

Notre systÃ¨me repose sur un modÃ¨le **XGBoost** entraÃ®nÃ© sur un vaste corpus de textes authentiques et gÃ©nÃ©rÃ©s par IA.

**Performances du modÃ¨le sur de nouvelles donnÃ©es :**
- **Accuracy** : XX.XX%
- **Precision** : XX.XX%
- **Recall** : XX.XX%
- **F1-Score** : XX.XX%

**Dataset d'entraÃ®nement :**
- XXX avis Ã©tiquetÃ©s (XXX humains / XXX IA)
- Source : Kaggle (liens vers les datasets)
- Langue : Anglais (uniquement, non?)
- Taille variable : De courts commentaires Ã  des essais dÃ©taillÃ©s

### ğŸ’¡ Cas d'usage

- **E-commerce** : Validation de l'authenticitÃ© des avis produits
- **HÃ´tellerie** : DÃ©tection de faux avis sur les plateformes de rÃ©servation  
- **ModÃ©ration** : Filtrage automatique des contenus gÃ©nÃ©rÃ©s automatiquement par IA
- **Analyse qualitative** : Audit de bases de donnÃ©es d'avis existantes

### ğŸš€ FonctionnalitÃ©s

Cette API propose deux modes d'utilisation :

1. **Analyse unitaire** : DÃ©tection pour un seul texte
2. **Traitement par lot** : Analyse de jusqu'Ã  10,000 textes simultanÃ©ment pour un traitement efficace

### ğŸ“Š Format des donnÃ©es

**EntrÃ©e :** Texte brut (sans limite de longueur)  
**Sortie :** Classification binaire (1 = IA ou 0 = Humain)

### âš¡ Performance

Temps de rÃ©ponse moyen : < XXXms par texte
"""

app = FastAPI(
    title="AI Review Detector API",
    description=description,
    version="1.0.0",
    contact={
        "name": "AI Review Detector Team",
        "email": "contact@aireviewdetector.com",
    }
)



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                                MODELES DE DONNEES
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#ModÃ¨le de donnÃ©es pour une requÃªte unitaire
class TextInput(BaseModel):
    text: str = Field(
        ...,
        description="Texte de l'avis Ã  analyser",
        example="This product exceeded my expectations! The quality is outstanding and delivery was super fast"
    )

#ModÃ¨le de donnÃ©es pour une requÃªte batch
class BatchTextInput(BaseModel):
    texts: Union[list[str], list[dict]] = Field(
        ...,
        description="Liste de textes Ã  analyser (maximum 10,000)",
        max_length=10000,
        example=[
            "This product exceeded my expectations! The quality is outstanding.",
            {"text": "I really enjoyed my stay at this hotel. The staff was very friendly."},
            "The food was delicious and the service was impeccable."
        ]
    )

#ModÃ¨le de donnÃ©es pour une rÃ©ponse unitaire
class PredictionResponse(BaseModel):
    is_ai_generated: int = Field(..., description="1 si gÃ©nÃ©rÃ© par l'IA, 0 si Ã©crit par un humain")
    message: str = Field(..., description="Message explicatif du rÃ©sultat")

#ModÃ¨le de donnÃ©es pour une rÃ©ponse batch
class BatchPredictionResponse(BaseModel):
    predictions: list[int] = Field(..., description="Liste des prÃ©dictions (1 = IA, 0 = Humain)")


#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               CONFIGURATION GLOBALE
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#Variables globales pour les ressources NLP
nlp = None
stop_en = None
tokenizer = None
model = None

#MLflow
MLFLOW_TRACKING_URI = "https://name-space_name.hf.space"
MODEL_URI = "models:/name_model/number_version"

#Configuration
STOPWORDS_LANGUAGE = "english"
BERT_MODEL_NAME = "bert-base-uncased"

#Listes de ponctuations et connecteurs
PUNCT_LIST = ['!', '?', ',', '.', ';', ':', '"', "'", '(']
ELLIPSIS_TOKEN = '...'

ALL_POS_TAG = [
    "DET", "VERB", "SCONJ", "AUX", "PART", "CCONJ",
    "ADV", "ADJ", "ADP", "PROPN", "PRON", "NOUN", "NUM"
]

connectives = {
    'addition': {'and', 'also', 'furthermore', 'moreover', 'in addition', 'besides', 'as well', 'what is more',
                 'not only... but also', 'similarly', 'likewise'},
    'contrast': {'but', 'however', 'on the other hand', 'nevertheless', 'nonetheless', 'yet', 'still',
                 'even so', 'although', 'though', 'whereas', 'while', 'in contrast', 'conversely'},
    'cause': {'because', 'since', 'as', 'due to', 'owing to', 'thanks to', 'considering that', 'for the reason that'},
    'consequence': {'so', 'therefore', 'thus', 'hence', 'as a result', 'consequently', 'accordingly', 'for this reason'},
    'concession': {'although', 'even though', 'though', 'while', 'granted that', 'admittedly', 'it is true that', 'nonetheless'},
    'example': {'for example', 'for instance', 'such as', 'like', 'to illustrate', 'namely', 'including', 'in particular'},
    'purpose': {'so that', 'in order to', 'in order that', 'so as to', 'to', 'for the purpose of', 'with the aim of'},
    'time': {'first', 'then', 'next', 'after that', 'afterwards', 'before', 'finally', 'meanwhile',
             'eventually', 'at the same time', 'subsequently'},
    'summary': {'in conclusion', 'to conclude', 'in summary', 'to sum up', 'overall', 'in short', 'all in all', 'ultimately'}
}



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           CHARGEMENT DES RESSOURCES NLP
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_nlp_ressources():
    """
    Charge toutes les ressources NLP nÃ©cessaires (Spacy, NLTK, BERT).
    Ã€ appeler UNE SEULE FOIS au dÃ©marrage de l'API.
    """

    global nlp, stop_en, tokenizer

    try:
        #TÃ©lÃ©charger les stopwords si nÃ©cessaire
        try:
            stopwords.words(STOPWORDS_LANGUAGE)
        except LookupError:
            nltk.download('stopwords', quiet=True)

        #Charger les stopwords
        stop_en = set(stopwords.words(STOPWORDS_LANGUAGE))

        #Charger Spacy
        nlp = spacy.load("en_core_web_sm", disable=["ner", "lemmatizer"])

        #Charger le tokenizer BERT
        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)

        print("âœ… Ressources NLP chargÃ©es avec succÃ¨s")
        return True
    
    except Exception as e:
        print(f"âŒ Erreur lors du chargement des ressources NLP : {e}")
        raise RuntimeError(f"Impossible de charger les ressources NLP : {e}")



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                        FONCTION DE PREPROCESSING / EXTRACTION
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def is_word_tok(tok):
    """
    VÃ©rifie si un token est un 'vrai mort' (alpha, nombre, URL)
    """
    return tok.is_alpha or tok.like_num or tok.like_url


def punctuation_features(text, len_words_value):
    """
    Extrait les features de ponctuation.
    """
    text = text or ""
    res = {}

    #Ellipses
    ellipses = text.count(ELLIPSIS_TOKEN)
    text_wo_ell = text.replace(ELLIPSIS_TOKEN, "")
    denominator = max(len_words_value, 1)
    res['punct_ellipsis_ratio'] = ellipses / denominator

    #Autres ponctuations
    for p in PUNCT_LIST:
        clean_p = re.sub(r'\W', '_', p)
        key = f"punct_{clean_p}_ratio"
        count = text_wo_ell.count(p)
        res[key] = count / denominator

    return res


def detect_connectives(text, len_word_value):
    """
    DÃ©tecte les connecteurs logiques.
    """
    text_lower = text.lower()
    detected = defaultdict(int)

    for category, phrases in connectives.items():
        for phrase in phrases:
            if phrase in text_lower:
                detected[category] += 1

    #Convertir en ratio
    denominator = max(len_word_value, 1)
    
    return {
        f"connective_{cat}_ratio": count / denominator for cat, count in detected.items()
    }


def pos_features(doc, len_words_value):
    """
    Extrait les features POS (Part-Of-Speech).
    """
    pos_counts = Counter(tok.pos_ for tok in doc if not tok.is_space)
    denominator = max(len_words_value, 1)

    return {
        f"pos_{pos}_ratio": pos_counts.get(pos, 0) / denominator for pos in ALL_POS_TAG
    }


def preprocessor(text):
    """
    PrÃ©traite le texte et extrait les features nÃ©cessaires au modÃ¨le.

    Args:
        text (str): Texte brut Ã  analyser

    Returns:
        pd.DataFrame: Ligne unique contenant toutes les features extraites
    """
    if not text or not isinstance(text, str):
        raise ValueError("Le texte doit Ãªtre une chaÃ®ne de caractÃ¨re non vide.")
    
    features = {}

    #Traitement avec Spacy
    doc = nlp(text)
    tokens = list(doc)

    #Mots valides
    valid_words = [tok for tok in tokens if is_word_tok(tok)]
    len_words = max(len(valid_words), 1) #Eviter les divisions par 0

    # === 1. Features de longueur ===
    features['len_chars'] = len(text)
    features['len_tokens_all'] = len(tokens)
    features['len_words'] = len_words

    #Phrases
    sentences = list(doc.sents)
    n_sentences = max(len(sentences), 1)
    features['n_sentences'] = n_sentences
    features['average_sentences_length'] = sum(len(
        [t for t in sent if not t.is_space ]) for sent in sentences) / n_sentences
    
    #Ratios
    features['len_chars_per_word'] = features['len_chars'] / len_words
    features['len_tokens_per_word'] = features['len_tokens_all'] /len_words

    # === 2. Ratios majuscules ===
    nb_uppercase = sum(1 for c in text if c.isupper())
    features['freq_uppercase'] = nb_uppercase / max(len(text), 1)

    # === 3. Features de ponctuation ===
    punct_features = punctuation_features(text, len_words)
    features.update(punct_features)

    # === 4. Ratio stopwords ===
    stopword_count = sum(1 for tok in valid_words if tok.text.lower() in stop_en)
    features['stopwords_ratio'] = stopword_count / len_words

    # === 5. Connecteurs logiques ===
    connective_features = detect_connectives(text, len_words)
    #Ajouter toutes les catÃ©gories (mÃªme si 0)
    for cat in connectives.keys():
        key = f"connective_{cat}_ratio"
        features['key'] = connective_features.get(key, 0.0)

    # === 6. POS tags ===
    pos_feat = pos_features(doc, len_words)
    features.update(pos_feat)

    #Convertir en DataFrame (une seule ligne)
    return pd.DataFrame([features])



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                        CHARGEMENT DU MODELE DE PREDICTION
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_ml_model():
    """
    Charge le modÃ¨le de ML depuis MLflow.
    Ã€ appeler UNE SEULE FOIS au dÃ©marrage de l'API.
    """

    global model

    try:
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        model = mlflow.sklearn.load_model(MODEL_URI)

        print("âœ… ModÃ¨le ML chargÃ© avec succÃ¨s")
        return True
    
    except Exception as e:
        print(f"âŒ Erreur lors du chargement du modÃ¨le : {e}")
        raise RuntimeError(f"Impossible de charger le modÃ¨le : {e}")



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                                   ENDPOINTS
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/", tags=["Root"])
async def root():
    """
    Point d'entrÃ©e de l'API.
    
    Retourne les informations de base sur le service.
    """
    return "Retrouvez toute la documentation de l'API sur /docs"


@app.on_event("startup")
async def startup_event():
    """
    Chargement des ressources au dÃ©marrage de l'API.
    """
    print("ğŸš€ DÃ©marrage de l'API...")

    try:
        #Charger les ressources NLP
        load_nlp_ressources()

        #Charger le modÃ¨le
        load_ml_model()

        print("âœ… API prÃªte Ã  recevoir des requÃªtes")

    except Exception as e:
        print(f"âŒ ERREUR CRITIQUE au dÃ©marrage : {e}")
        raise


@app.post("/predict", response_model=PredictionResponse, tags=["DÃ©tection"])
async def predict_single_text(input_data: TextInput):
    """
    Analyse un seul texte pour dÃ©terminÃ© s'il a Ã©tÃ© gÃ©nÃ©rÃ© par une IA.

    **Format acceptÃ© pour la requÃªte :**
    json:
    `{
        "text": "Ici se trouve le texte."
    }`

    **Format acceptÃ© pour le champ "text" :**
    str `"Voici le texte."`

    **Retourne :**
    - `is_ai_generated` : 1 si le texte est gÃ©nÃ©rÃ© par IA, 0 s'il est Ã©crit par un humain

    - `message` : Message explicatif en langage naturel
    """

    #Preprocessing
    features_df = preprocessor(input_data.text)

    #PrÃ©diction
    prediction = int(model.predict(features_df)[0])

    #Message explicatif
    if prediction == 1:
        message = "Cet avis semble avoir Ã©tÃ© gÃ©nÃ©rÃ© par une intelligence artificielle."
    else:
        message = "Ce texte semble avoir Ã©tÃ© Ã©crit par un humain."

    return PredictionResponse(
        is_ai_generated=prediction,
        message=message
    )


@app.post("/predict-batch", response_model=BatchPredictionResponse, tags=["DÃ©tection"])
async def predict_batch_texts(input_data: BatchTextInput):
    """
    Analyse plusieurs textes simultanÃ©ment pour dÃ©terminer s'ils ont Ã©tÃ© gÃ©nÃ©rÃ©s par une IA.

    **Limite :** Maximum 10,000 textes par requÃªte

    **Format de la requÃªte:**
    json:
    `{
        "texts": ["text1", "text2", ...]
    }`

    **Formats acceptÃ©s pour le champ "texts" :**
    - Liste de strings: `["text1", "text2", ...]`

    - Liste de dictionnaires: `[{"text": "text1}, {"texte": "text2}, ...]`

    **Retourne :**
    `predictions` : Liste des prÃ©dictions (1 = IA, 0 = Humain) dans le mÃªme ordre que les textes soumis
    """

    #Extraction des textes selon le format
    texts = []
    for item in input_data.texts:
        #Si c'est une liste de textes
        if isinstance(item, str):
            texts.append(item)
        #Sinon, si c'est une liste de dictionnaire
        elif isinstance(item, dict):
            #Et si ce dictionnaire contient une clÃ© "text"
            if "text" in item:
                texts.append(item["text"])
            #Ou sinon si ce dictionnaire contient une clÃ© "texte"
            elif "texte" in item:
                texts.append(item['texte'])
            #Sinon, on lÃ¨ve une erreur
            else:
                raise HTTPException(
                    status_code=400,
                    detail="Les dictionnaires doivent contenir une clÃ© 'texte."
                )
        #Si ce n'est ni une liste de texte ni une liste de dictionnaire, on lÃ¨ve une erreur
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Format non supportÃ©: {type(item)}. Utilisez des strings ou des dictionnaires."
            )

    #Preprocessing
    features_list = pd.concat([preprocessor(text) for text in texts], ignore_index=True)

    #Predictions
    predictions = model.predict(features_list)

    #Conversion en liste
    predictions = predictions.tolist() if hasattr(predictions, 'tolist') else list(predictions)

    return BatchPredictionResponse(predictions=predictions)